% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sits_get_data.R
\name{sits_get_data}
\alias{sits_get_data}
\alias{sits_get_data.default}
\alias{sits_get_data.csv}
\alias{sits_get_data.shp}
\alias{sits_get_data.sf}
\alias{sits_get_data.sits}
\alias{sits_get_data.data.frame}
\title{Get time series from data cubes and cloud services}
\usage{
sits_get_data(cube, samples, ...)

\method{sits_get_data}{default}(cube, samples, ...)

\method{sits_get_data}{csv}(
  cube,
  samples,
  ...,
  bands = NULL,
  crs = "EPSG:4326",
  impute_fn = impute_linear(),
  multicores = 2,
  progress = FALSE
)

\method{sits_get_data}{shp}(
  cube,
  samples,
  ...,
  label = "NoClass",
  start_date = NULL,
  end_date = NULL,
  bands = NULL,
  impute_fn = impute_linear(),
  label_attr = NULL,
  n_sam_pol = 30,
  pol_avg = FALSE,
  sampling_type = "random",
  multicores = 2,
  progress = FALSE
)

\method{sits_get_data}{sf}(
  cube,
  samples,
  ...,
  start_date = NULL,
  end_date = NULL,
  bands = NULL,
  impute_fn = impute_linear(),
  label = "NoClass",
  label_attr = NULL,
  n_sam_pol = 30,
  pol_avg = FALSE,
  sampling_type = "random",
  multicores = 2,
  progress = FALSE
)

\method{sits_get_data}{sits}(
  cube,
  samples,
  ...,
  bands = NULL,
  crs = "EPSG:4326",
  impute_fn = impute_linear(),
  multicores = 2,
  progress = FALSE
)

\method{sits_get_data}{data.frame}(
  cube,
  samples,
  ...,
  start_date = NULL,
  end_date = NULL,
  bands = NULL,
  label = "NoClass",
  crs = "EPSG:4326",
  impute_fn = impute_linear(),
  multicores = 2,
  progress = FALSE
)
}
\arguments{
\item{cube}{Data cube from where data is to be retrieved.
(tibble of class "raster_cube").}

\item{samples}{Location of the samples to be retrieved.
Either a tibble of class "sits", an "sf" object,
the name of a shapefile or csv file, or
a data.frame with columns "longitude" and "latitude".}

\item{...}{Specific parameters for specific cases.}

\item{bands}{Bands to be retrieved - optional
(character vector).}

\item{crs}{Default crs for the samples
(character vector of length 1).}

\item{impute_fn}{Imputation function to remove NA.}

\item{multicores}{Number of threads to process the time series
(integer, with min = 1 and max = 2048).}

\item{progress}{Logical: show progress bar?}

\item{label}{Label to be assigned to the time series (optional)
(character vector of length 1).}

\item{start_date}{Start of the interval for the time series - optional
(Date in "YYYY-MM-DD" format).}

\item{end_date}{End of the interval for the time series - optional
(Date in "YYYY-MM-DD" format).}

\item{label_attr}{Attribute in the shapefile or sf object to be used
as a polygon label.
(character vector of length 1).}

\item{n_sam_pol}{Number of samples per polygon to be read
for POLYGON or MULTIPOLYGON shapefiles or sf objects
(single integer).}

\item{pol_avg}{Logical: summarize samples for each polygon?
(character vector of length 1)}

\item{sampling_type}{Spatial sampling type: random, hexagonal,
regular, or Fibonacci.}
}
\value{
A tibble of class "sits" with set of time series
<longitude, latitude, start_date, end_date, label>.
}
\description{
Retrieve a set of time series from a data cube and
and put the result in a "sits tibble", which
contains both the satellite image time series and their metadata.
}
\note{
The main \code{sits} classification workflow has the following steps:
\enumerate{
     \item{\code{\link[sits]{sits_cube}}: selects a ARD image collection from
         a cloud provider.}
     \item{\code{\link[sits]{sits_cube_copy}}: copies the ARD image collection
         from a cloud provider to a local directory for faster processing.}
     \item{\code{\link[sits]{sits_regularize}}: create a regular data cube
         from an ARD image collection.}
     \item{\code{\link[sits]{sits_apply}}: create new indices by combining
         bands of a  regular data cube (optional).}
     \item{\code{\link[sits]{sits_get_data}}: extract time series
         from a regular data cube based on user-provided labelled samples.}
     \item{\code{\link[sits]{sits_train}}: train a machine learning
         model based on image time series.}
     \item{\code{\link[sits]{sits_classify}}: classify a data cube
         using a machine learning model and obtain a probability cube.}
     \item{\code{\link[sits]{sits_smooth}}: post-process a probability cube
         using a spatial smoother to remove outliers and
         increase spatial consistency.}
     \item{\code{\link[sits]{sits_label_classification}}: produce a
         classified map by selecting the label with the highest probability
         from a smoothed cube.}
}

To be able to build a machine learning model to classify a data cube,
one needs to use a set of labelled time series. These time series
are created by taking a set of known samples, expressed as
labelled points or polygons.
This \code{sits_get_data} function uses these samples to
extract time series from a data cube. Thus, it needs a \code{cube} parameter
which points to a regularized data cube, and a \code{samples} parameter
that describes the locations of the training set.

There are five ways of specifying the
\code{samples} parameter:
\enumerate{
\item{A CSV file with columns
\code{longitude}, \code{latitude},
\code{start_date}, \code{end_date} and \code{label} for each sample.
The parameter must point to a file with extension ".csv";}
\item{A shapefile in POINT or POLYGON geometry
containing the location of the samples.
The parameter must point to a file with extension ".shp";}
\item{A sits tibble, which contains columns
\code{longitude}, \code{latitude},
\code{start_date}, \code{end_date} and \code{label} for each sample.}
\item{A \code{link[sf]{sf}} object with POINT or POLYGON geometry;}
\item{A data.frame with with mandatory columns
\code{longitude}, \code{latitude},
\code{start_date}, \code{end_date} and \code{label} for each row.}
}

For shapefiles and sf objects, the following parameters are relevant:
\enumerate{
\item{\code{label}: label to be assigned to the samples.
Should only be used if all geometries have a single label.}
\item{\code{label_attr}: defines which attribute should be
used as a label, required for POINT and POLYGON geometries if
\code{label} has not been set.}
\item{\code{n_sam_pol}: indicates how many points are
extracted from each polygon, required for POLYGON geometry (default = 15).}
\item{\code{sampling_type}: defines how sampling is done, required
for POLYGON geometry (default = "random").}
\item{\code{pol_avg}: indicates if average of values for POLYGON
geometry should be computed (default = "FALSE").}
}
}
\examples{
if (sits_run_examples()) {
    # reading a lat/long from a local cube
    # create a cube from local files
    data_dir <- system.file("extdata/raster/mod13q1", package = "sits")
    raster_cube <- sits_cube(
        source = "BDC",
        collection = "MOD13Q1-6.1",
        data_dir = data_dir
    )
    samples <- tibble::tibble(longitude = -55.66738, latitude = -11.76990)
    point_ndvi <- sits_get_data(raster_cube, samples)
    #
    # reading samples from a cube based on a  CSV file
    csv_file <- system.file("extdata/samples/samples_sinop_crop.csv",
        package = "sits"
    )
    points <- sits_get_data(cube = raster_cube, samples = csv_file)

    # reading a shapefile from BDC (Brazil Data Cube)
    bdc_cube <- sits_cube(
            source = "BDC",
            collection = "CBERS-WFI-16D",
            bands = c("NDVI", "EVI"),
            tiles = c("007004", "007005"),
            start_date = "2018-09-01",
            end_date = "2018-10-28"
    )
    # define a shapefile to be read from the cube
    shp_file <- system.file("extdata/shapefiles/bdc-test/samples.shp",
            package = "sits"
    )
    # get samples from the BDC based on the shapefile
    time_series_bdc <- sits_get_data(
        cube = bdc_cube,
        samples = shp_file
    )
}

}
\author{
Felipe Carlos, \email{efelipecarlos@gmail.com}

Felipe Carvalho, \email{felipe.carvalho@inpe.br}

Gilberto Camara, \email{gilberto.camara@inpe.br}

Rolf Simoes, \email{rolfsimoes@gmail.com}
}

% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sits_torch_optim_adabound.R
\name{optim_adabound}
\alias{optim_adabound}
\title{Adabound optimizer}
\usage{
optim_adabound(params, defaults)
}
\arguments{
\item{params}{List of parameters to optimize.}

\item{lr}{Learning rate (default: 1e-3)}

\item{betas}{Coefficients computing running averages of gradient
and its square (default: (0.9, 0.999))}

\item{final_lr}{Final (SGD) learning rate (default: 0.1)}

\item{gamma}{Convergence speed of the bound functions (default: 1e-3)}

\item{eps}{Term added to the denominator to improve numerical stability
(default: 1e-8)}

\item{weight_decay}{Weight decay (L2 penalty) (default: 0)}
}
\description{
R implementation of the Adabound optimizer proposed
by Luo et al.(2019). The original implementation is available at
https://github.com/Luolc/AdaBound.

AdaBound is a variant of the Adam stochastic optimizer which is
designed to be more robust to extreme learning rates.
Dynamic bounds are employed on learning rates,
where the lower and upper bound are initialized as zero and
infinity respectively, and they both smoothly converge to a
constant final step size. AdaBound can be regarded as an adaptive
method at the beginning of training, and thereafter it gradually and
smoothly transforms to SGD (or with momentum) as the time step increases.
}
\references{
Liangchen Luo, Yuanhao Xiong, Yan Liu, Xu Sun,
"Adaptive Gradient Methods with Dynamic Bound of Learning Rate",
International Conference on Learning Representations (ICLR), 2019.
https://arxiv.org/abs/1902.09843
}
\author{
Rolf Simoes, \email{rolf.simoes@inpe.br}

Felipe Souza, \email{lipecaso@gmail.com}

Alber Sanchez, \email{alber.ipia@inpe.br}

Gilberto Camara, \email{gilberto.camara@inpe.br}
}

% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sits_tuning.R
\name{sits_tuning}
\alias{sits_tuning}
\title{Tuning deep learning models hyper-parameters}
\usage{
sits_tuning(
  samples,
  samples_validation = NULL,
  validation_split = 0.2,
  ml_functions = list(sits_tempcnn, sits_lighttae),
  opt_functions = list(torch::optim_adam, optim_adamw),
  learning_rates = c(0.01, 0.005, 0.001),
  eps_values = c(1e-06, 1e-07, 1e-08),
  weight_decays = c(0, 1e-05, 1e-06),
  multicores = 2
)
}
\arguments{
\item{samples}{Time series set to be validated.}

\item{samples_validation}{Time series set used for validation.}

\item{validation_split}{Percent of original time series set to be used
for validation (if samples_validation is NULL)}

\item{ml_functions}{List of machine learning functions to be tested.}

\item{opt_functions}{Optimization functions to be tested.}

\item{learning_rates}{Learning rates to be tested.}

\item{eps_values}{Values of eps to be tested. Each eps value
is the term added to the denominator to
improve numerical stability.}

\item{weight_decays}{Values of weight decay to be tested.
These are the the L2 regularization params
(note the weight decay is not correctly
implemented in the adam optimization)}

\item{multicores}{Multicores to be used}
}
\value{
A list containing the best torch optimizer and its parameters.
}
\description{
Deep learning models use stochastic gradient descent techniques to
find optimal solutions. To that end, these models use optimization
algorithms that approximate the actual solution, which would be
computationally expensive. Each of these algorithms uses a set of
hyperparameters, that have to be adjusted to achieve best performance
for each application.
This function combines all parameters and computes torch models to
parameter combination, do a validation using validation samples or
splitting samples using validation_split. The function returns the
best hyper-parameters in a list.
}
\examples{
\donttest{

# tuning cerrado samples
hparams <- sits_tuning(samples        = cerrado_2classes,
                             ml_functions   = list(sits_tempcnn),
                             opt_functions  = list(torch::optim_adam),
                             learning_rates = c(0.005, 0.001),
                             eps_values     = 1e-06,
                             weight_decays  = 0)
}
}
